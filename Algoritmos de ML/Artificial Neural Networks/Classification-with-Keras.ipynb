{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION MODELS WITH KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b909ae-4a8f-4bde-a66b-090afd8151eb",
   "metadata": {},
   "source": [
    "## Objetivos   \n",
    "\n",
    "* Usar la database de MNIST para entrenar varios sistemas de procesado de imágenes\n",
    "* Construir una red neuronal\n",
    "* Entrenarla y testearla\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0382f2-f135-4870-9d71-212e59c9f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c0174a-5eef-4c5f-8a9c-54c150f90ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe0e17-5d41-4dfb-8c57-b86293309403",
   "metadata": {},
   "source": [
    "Cargamos el dataset MNIST de la libería de Keras el cual ya está dividio en set de entrenamiento y set de prueba.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb035b3-f6b0-455c-9c43-e7445d3a42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# importar los datos\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# cargarlos\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca734f-7ccb-4ec6-9dcc-47462d8db865",
   "metadata": {},
   "source": [
    "\n",
    "Vamos a confirmar el número de imágenes en cada set (atendiendo a la documentación tiene que haber 60.000 imágenes en X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c93f66-dac7-4937-ae8f-2f346e6dd258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9c1ec-94a1-4545-9205-133bd8476ee0",
   "metadata": {},
   "source": [
    "\n",
    "El primer número hace referencia al número de imágenes, y los otros dos el tamaño de las mismas (28 x 28 pixeles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b2825-42a0-4052-b0d5-5ae02dc5e859",
   "metadata": {},
   "source": [
    "Visualizamos la primera imágen para tener una idea clara: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755619b2-64e3-416d-922b-8e6b8526d05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a72c53f390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f703ea-874d-47dd-9d7a-20c78cc13b36",
   "metadata": {},
   "source": [
    "\n",
    "Con redes neuronales convencionales, no podemos meter imágenes como inputs asi que las transformamos en vectores de una dimensión: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a137c1-3554-4027-86f0-b1409ba810f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2] # encontrar el tamaño del vector\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32') # aplicar a imágenes de entrenamiento\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32') # aplicar a imágenes de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da865046-1d25-4855-bb8d-3b6bfb15343a",
   "metadata": {},
   "source": [
    "\n",
    "Como cada valor del pixel puede ir de 0 a 255, vamos a normalizar los vectores entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b71bef-dd01-4e76-914e-f97e280b3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba8319-79d1-4079-9afd-047692d9f3e4",
   "metadata": {},
   "source": [
    "\n",
    "Por último, empezamos a construir el modelo. Para la clasificación necesitaremos dividir nuestra variable target en categorías: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "416635d1-1051-4b66-9acb-8dd5b8e68290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ce88d-0157-43f2-b64f-5b09fd916c78",
   "metadata": {},
   "source": [
    "## Construir una red Neuronal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f4c784-d600-4b17-b494-3e98ee07af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_model():\n",
    "    # crear el modelo\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(num_pixels,)))\n",
    "    model.add(Dense(num_pixels, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d7bc7-1344-4d0d-857b-4a91c685c232",
   "metadata": {},
   "source": [
    "## Entrenar y testear la red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711b258f-73fe-4ace-ae96-e72c71d6af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 13s - 7ms/step - accuracy: 0.9440 - loss: 0.1869 - val_accuracy: 0.9670 - val_loss: 0.1062\n",
      "Epoch 2/10\n",
      "1875/1875 - 14s - 7ms/step - accuracy: 0.9751 - loss: 0.0787 - val_accuracy: 0.9739 - val_loss: 0.0843\n",
      "Epoch 3/10\n",
      "1875/1875 - 16s - 9ms/step - accuracy: 0.9830 - loss: 0.0539 - val_accuracy: 0.9771 - val_loss: 0.0735\n",
      "Epoch 4/10\n",
      "1875/1875 - 18s - 10ms/step - accuracy: 0.9872 - loss: 0.0392 - val_accuracy: 0.9770 - val_loss: 0.0784\n",
      "Epoch 5/10\n",
      "1875/1875 - 17s - 9ms/step - accuracy: 0.9899 - loss: 0.0316 - val_accuracy: 0.9793 - val_loss: 0.0783\n",
      "Epoch 6/10\n",
      "1875/1875 - 16s - 9ms/step - accuracy: 0.9916 - loss: 0.0252 - val_accuracy: 0.9776 - val_loss: 0.0892\n",
      "Epoch 7/10\n",
      "1875/1875 - 18s - 9ms/step - accuracy: 0.9929 - loss: 0.0230 - val_accuracy: 0.9793 - val_loss: 0.0826\n",
      "Epoch 8/10\n",
      "1875/1875 - 17s - 9ms/step - accuracy: 0.9933 - loss: 0.0209 - val_accuracy: 0.9776 - val_loss: 0.1015\n",
      "Epoch 9/10\n",
      "1875/1875 - 17s - 9ms/step - accuracy: 0.9944 - loss: 0.0174 - val_accuracy: 0.9822 - val_loss: 0.0890\n",
      "Epoch 10/10\n",
      "1875/1875 - 17s - 9ms/step - accuracy: 0.9946 - loss: 0.0160 - val_accuracy: 0.9823 - val_loss: 0.0944\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = classification_model()\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8668581-4e3e-4cae-a0ba-1932c560aa01",
   "metadata": {},
   "source": [
    "\n",
    "Mostramos la precisión y su error correspondiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62635ba-7b96-44d8-85e5-b93101313e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9822999835014343% \n",
      " Error: 0.017700016498565674\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb896d9-ac23-49e1-a08d-3e95a12f3baa",
   "metadata": {},
   "source": [
    "Just running 10 epochs could actually take over 20 minutes. But enjoy the results as they are getting generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fbd92-dfb1-49c5-b8d4-a1c0a60a4b06",
   "metadata": {},
   "source": [
    "\n",
    "A veces, dependiendo de como sea nuestro modelo de complejo, nos será imposible reentrenarlo cada vez que lo queramos utilizar. Keras nos permite guardarlo después de entrenarlo para luego poder usarlo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26f2fd96-8a5e-4323-9fa2-0102ad9cabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d23cda-d3df-451f-bb71-087284ef482d",
   "metadata": {},
   "source": [
    "\n",
    "Cuando queramos usarlo solo hay que llamar al método load_model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30487ebb-692c-4cdc-b3ea-ac0df17cfcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras.saving.load_model('classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a77d59-6abd-4025-ac75-2e890c84a72b",
   "metadata": {},
   "source": [
    "### Ejercicio Práctico 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4499a9-fdaa-4c5e-8a44-1332791440a0",
   "metadata": {},
   "source": [
    "\n",
    "Crear una red neuronal con 6 layers y comparar las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c920a54-1d95-4f5a-8f3a-95f1f933a775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 17s - 9ms/step - accuracy: 0.9317 - loss: 0.2252 - val_accuracy: 0.9652 - val_loss: 0.1195\n",
      "Epoch 2/10\n",
      "1875/1875 - 18s - 9ms/step - accuracy: 0.9705 - loss: 0.1003 - val_accuracy: 0.9686 - val_loss: 0.1123\n",
      "Epoch 3/10\n",
      "1875/1875 - 19s - 10ms/step - accuracy: 0.9782 - loss: 0.0721 - val_accuracy: 0.9746 - val_loss: 0.0950\n",
      "Epoch 4/10\n",
      "1875/1875 - 23s - 12ms/step - accuracy: 0.9831 - loss: 0.0569 - val_accuracy: 0.9776 - val_loss: 0.0779\n",
      "Epoch 5/10\n",
      "1875/1875 - 20s - 11ms/step - accuracy: 0.9870 - loss: 0.0446 - val_accuracy: 0.9765 - val_loss: 0.1127\n",
      "Epoch 6/10\n",
      "1875/1875 - 22s - 12ms/step - accuracy: 0.9884 - loss: 0.0400 - val_accuracy: 0.9753 - val_loss: 0.1009\n",
      "Epoch 7/10\n",
      "1875/1875 - 38s - 20ms/step - accuracy: 0.9900 - loss: 0.0339 - val_accuracy: 0.9787 - val_loss: 0.0773\n",
      "Epoch 8/10\n",
      "1875/1875 - 18s - 9ms/step - accuracy: 0.9913 - loss: 0.0301 - val_accuracy: 0.9788 - val_loss: 0.0844\n",
      "Epoch 9/10\n",
      "1875/1875 - 25s - 13ms/step - accuracy: 0.9926 - loss: 0.0251 - val_accuracy: 0.9816 - val_loss: 0.0923\n",
      "Epoch 10/10\n",
      "1875/1875 - 19s - 10ms/step - accuracy: 0.9925 - loss: 0.0271 - val_accuracy: 0.9824 - val_loss: 0.0854\n",
      "Accuracy_3_layers: 0.9822999835014343% \n",
      " Accuracy_6_layers: 0.9824000000953674\n"
     ]
    }
   ],
   "source": [
    "def classification_model():\n",
    "    # crear el modelo\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(num_pixels,)))\n",
    "    model.add(Dense(num_pixels, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model_6layers= classification_model()\n",
    "\n",
    "model_6layers.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
    "\n",
    "scores_6layers = model_6layers.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Accuracy_3_layers: {}% \\n Accuracy_6_layers: {}'.format(scores[1], scores_6layers[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06868e1f-5543-4536-8ed3-ebbf3715663f",
   "metadata": {},
   "source": [
    "### Ejercicio Práctico 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cca9e-a280-4083-9399-536639edcc00",
   "metadata": {},
   "source": [
    "\n",
    "Ahora, cargamos el modelo entrenado que hemos guardardo, lo entrenamos con 10 épocas más y comprobamos la precisión: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02fdd930-699f-4483-842e-38f66775fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model loaded successufully\n",
      "Epoch 1/10\n",
      "1875/1875 - 13s - 7ms/step - accuracy: 0.9958 - loss: 0.0141 - val_accuracy: 0.9816 - val_loss: 0.0892\n",
      "Epoch 2/10\n",
      "1875/1875 - 13s - 7ms/step - accuracy: 0.9957 - loss: 0.0135 - val_accuracy: 0.9776 - val_loss: 0.1035\n",
      "Epoch 3/10\n",
      "1875/1875 - 13s - 7ms/step - accuracy: 0.9958 - loss: 0.0142 - val_accuracy: 0.9819 - val_loss: 0.0949\n",
      "Epoch 4/10\n",
      "1875/1875 - 14s - 8ms/step - accuracy: 0.9964 - loss: 0.0123 - val_accuracy: 0.9803 - val_loss: 0.1045\n",
      "Epoch 5/10\n",
      "1875/1875 - 14s - 7ms/step - accuracy: 0.9961 - loss: 0.0138 - val_accuracy: 0.9830 - val_loss: 0.1086\n",
      "Epoch 6/10\n",
      "1875/1875 - 13s - 7ms/step - accuracy: 0.9972 - loss: 0.0099 - val_accuracy: 0.9809 - val_loss: 0.1111\n",
      "Epoch 7/10\n",
      "1875/1875 - 13s - 7ms/step - accuracy: 0.9968 - loss: 0.0111 - val_accuracy: 0.9820 - val_loss: 0.1078\n",
      "Epoch 8/10\n",
      "1875/1875 - 13s - 7ms/step - accuracy: 0.9976 - loss: 0.0086 - val_accuracy: 0.9842 - val_loss: 0.0925\n",
      "Epoch 9/10\n",
      "1875/1875 - 13s - 7ms/step - accuracy: 0.9970 - loss: 0.0118 - val_accuracy: 0.9819 - val_loss: 0.1221\n",
      "Epoch 10/10\n",
      "1875/1875 - 14s - 7ms/step - accuracy: 0.9969 - loss: 0.0106 - val_accuracy: 0.9823 - val_loss: 0.1166\n",
      "Accuracy_10_epochs: 0.9822999835014343% \n",
      " Accuracy_20_epochs: 0.9822999835014343\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "#load the saved model\n",
    "pretrained_model = keras.saving.load_model('classification_model.keras')\n",
    "\n",
    "print(\"Pre-trained model loaded successufully\")\n",
    "\n",
    "# Further train the loaded model\n",
    "pretrained_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "scores_20_epochs = pretrained_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy_10_epochs: {}% \\n Accuracy_20_epochs: {}'.format(scores[1], scores_20_epochs[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergencia Rápida: MNIST es un dataset que permite obtener precisiones superiores al 95% muy rápidamente (incluso en las primeras épocas) debido a la clara diferenciación de sus rasgos por lo que no se verá un gran aumento de precisión al aumentar el número de hidden layers o de épocas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "prev_pub_hash": "c850b639d611b84ea2eecf5ddb84ea433849e20b6766a84b43a9df0c51e4e9ee"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
